{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Frozen Lake.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "zbb6onm5Inxf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e4b4e33-3e6b-4737-88dc-403762a5cca7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "迭代次数为7\n",
            "[0. 3. 3. 3. 0. 0. 0. 0. 3. 1. 0. 0. 0. 2. 1. 0.]\n"
          ]
        }
      ],
      "source": [
        "'''用策略迭代解决冰湖问题'''\n",
        "'''问题描述见readme.md\n",
        "  求解目标：：智能体学习从起始块如何行动到目标块，并避开危险块。即确\n",
        "定智能体的最优行动路线。'''\n",
        "\n",
        "#导入解决该问题需要的库(基于Pytorch环境)\n",
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "#利用gym，创建冰湖环境\n",
        "env=gym.make('FrozenLake-v0')\n",
        "\n",
        "#设置最大迭代次数\n",
        "Iterations=1000000\n",
        "# set the threshold\n",
        "#设置误差，以判断策略迭代是否已经收敛\n",
        "Epsilon=1e-8\n",
        "#设置折扣因子\n",
        "Gama=1.0\n",
        "\n",
        "#通过贝尔曼方程计算价值函数\n",
        "def Value_Function(Policy,Gama):\n",
        "    #初始化价值表\n",
        "    Value_Table=np.zeros(env.nS)\n",
        "\n",
        "    while True:\n",
        "        #更新价值表\n",
        "        New_Value_Table=np.copy(Value_Table)\n",
        "        #在当前状态下，根据策略选择动作并计算价值表\n",
        "        for i in range(env.nS):\n",
        "            a=Policy[i]\n",
        "            #计算价值表\n",
        "            Value_Table[i] = sum([trans_prob * (reward_prob + Gama * New_Value_Table[next_state])\n",
        "                        for trans_prob, next_state, reward_prob, _ in env.P[i][a]])\n",
        "            #Value_Table[i]=sum([(reward_prob+Gama*New_Value_Table[next_state])*trans_prob]) for trans_prob,next_state,reward_prob,_ in env.P[i][a]])\n",
        "\n",
        "                \n",
        "\n",
        "        if(np.sum((np.fabs(New_Value_Table - Value_Table))) <= Epsilon):\n",
        "            break\n",
        "    return Value_Table\n",
        "\n",
        "#制定策略\n",
        "def Get_Policy(Value_Table,Gama):\n",
        "    Policy=np.zeros(env.observation_space.n)\n",
        "    for i in range(env.observation_space.n):\n",
        "        #构建并初始化Q表\n",
        "        Q=np.zeros(env.action_space.n)\n",
        "        #计算所有动作的Q值\n",
        "        for a in range(env.action_space.n):\n",
        "            for n in env.P[i][a]:\n",
        "                trans_prob, next_state, reward_prob, _ = n\n",
        "                Q[a]=Q[a]+(trans_prob * (reward_prob + Gama * Value_Table[next_state]))\n",
        "        #通过贝尔曼最优方程得到最佳动作\n",
        "        Policy[i]=np.argmax(Q)\n",
        "    return Policy\n",
        "\n",
        "def Policy_Itreration(env,Gama):\n",
        "    Gama=1.0\n",
        "    #初始化\n",
        "    O_Policy=np.zeros(env.observation_space.n)\n",
        "    #开始迭代并检查是否已收敛\n",
        "    for i in range(Iterations):\n",
        "        #价值函数计算\n",
        "        Value_Fun=Value_Function(O_Policy,Gama)\n",
        "        #新策略\n",
        "        N_Policy=Get_Policy(Value_Fun,Gama)\n",
        "        #检查是否已收敛\n",
        "        if(np.all(O_Policy==N_Policy)):\n",
        "            print('迭代次数为%d'%(i+1))\n",
        "            break\n",
        "        O_Policy=N_Policy\n",
        "    return N_Policy\n",
        "    \n",
        "#输出策略\n",
        "print('得到的策略如下')\n",
        "print(Policy_Itreration(env,Gama))"
      ]
    }
  ]
}