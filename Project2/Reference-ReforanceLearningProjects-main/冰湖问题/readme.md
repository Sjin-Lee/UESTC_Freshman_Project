## 1、问题描述

基于 Gym 环境中的 FrozenLake-v0，其环境如下图所示：

![冰](C:\Users\Aaron Summers\Desktop\冰.png)

FrozenLake-v0 是一个 4*4 的网络格子，每个格子可能是起始块（S）、目标块（G）、冻结块 （F）或危险块（H）。智能体可以选择向上、向下、向左或向右移动。智能体从起始块出发， 可以在冻结块上行走；而在危险块上会落水。同时，智能体的移动方向是不确定的，只是部 分依赖于智能体的决策方向：冰面是光滑的，所以你不会总是沿着目标方向移动。在这种情 况下，每个时刻都有完美的策略是不可实现的，但是如何避免危险块并到达目标块是可行的。 智能体达到目标块，或掉进危险块，则这一回合的游戏结束。如果达到了目标，即智能体找 到了一条从出发点到终点的可行路线，则智能体将得到 1 分的奖励，否则为 0 分。

智能体学习从起始块如何行动到目标块，并避开危险块。即确 定智能体的最优行动路线。

## 2、解决方法

使用动态规划的方法，在此问题中，可以通过实现Policy Iteration解决该问题。

PI的伪代码如下：

![](C:\Users\Aaron Summers\Desktop\PI.png)

智能体的行动方向表示如下：

​	向左：0

​	向下：1

​	向右：2

​	向上：3

代码中重要的参数：

- `env`：OpenAI Gym 环境的实例，`env.P` 会返回一步动态特性。
- `Policy`：维 numpy 数组，其中 `policy.shape[0]` 等于状态数量 (`env.nS`) ， `Policy.shape[1]` 等于动作数量 (`env.nA`) 。`policy[i][a]` 返回智能体在状态 `i` 时根据该策略选择动作 `a` 的概率。
- `Gama`：折扣率,在 0 到 1之间的值，默认值为：`1.0`。
- `Epsilon`：用于判断策略是否收敛 (默认值为：`1e-8`）。

## 3、结果展示

得到的最佳策略如下：
[0. 3. 3. 3. 0. 0. 0. 0. 3. 1. 0. 0. 0. 2. 1. 0.]

而得到该策略所用的迭代次数为7，7次即可让策略收敛。





