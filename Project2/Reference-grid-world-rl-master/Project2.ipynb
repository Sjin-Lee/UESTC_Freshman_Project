{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Project 2 Report\n",
    "\n",
    "##### Goal\n",
    "\n",
    "\n",
    "设计一个地图，有一些箱子（奖励）权重，智能体移动到每一个格子上都有一个可能性，所以有一个状态转移方程，尝试计算出最大收益。\n",
    "\n",
    "\n",
    "##### Design\n",
    "\n",
    "\n",
    "选题：Frozen Lake 冰湖问题\n",
    "\n",
    "\n",
    "##### 什么是Frozen Lake \n",
    "\n",
    "\n",
    "Frozen Lake 是指在一块冰面上有四种state：\n",
    "\n",
    "S: initial stat 起点\n",
    "\n",
    "F: frozen lake 冰湖\n",
    "\n",
    "H: hole 窟窿\n",
    "\n",
    "G: the goal 目的地\n",
    "\n",
    "agent 要学会从起点走到目的地，并且不要掉进窟窿。\n",
    "\n",
    "求解目标：：智能体学习从起始块如何行动到目标块，并避开危险块。即确\n",
    "定智能体的最优行动路线。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入解决该问题需要的库(基于Pytorch环境)\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#利用gym，创建冰湖环境\n",
    "env=gym.make('FrozenLake8x8-v0')\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#设置最大迭代次数\n",
    "Iterations=1000000\n",
    "# set the threshold\n",
    "#设置误差，以判断策略迭代是否已经收敛\n",
    "Epsilon=1e-8\n",
    "#设置折扣因子\n",
    "Gama=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#通过贝尔曼方程计算价值函数\n",
    "def Value_Function(Policy,Gama):\n",
    "    #初始化价值表\n",
    "    Value_Table=np.zeros(env.ns)\n",
    "\n",
    "    while True:\n",
    "        #更新价值表\n",
    "        New_Value_Table=np.copy(Value_Table)\n",
    "        #在当前状态下，根据策略选择动作并计算价值表\n",
    "        for i in range(env.nS):\n",
    "            a=Policy[i]\n",
    "            #计算价值表\n",
    "            Value_Table[i] = sum([trans_prob * (reward_prob + Gama * New_Value_Table[next_state])\n",
    "                        for trans_prob, next_state, reward_prob, _ in env.P[i][a]])\n",
    "            #Value_Table[i]=sum([(reward_prob+Gama*New_Value_Table[next_state])*trans_prob]) for trans_prob,next_state,reward_prob,_ in env.P[i][a]])\n",
    "\n",
    "                \n",
    "\n",
    "        if(np.sum((np.fabs(New_Value_Table - Value_Table))) <= Epsilon):\n",
    "            break\n",
    "    return Value_Table\n",
    "\n",
    "#制定策略\n",
    "def Get_Policy(Value_Table,Gama):\n",
    "    Policy=np.zeros(env.observation_space.n)\n",
    "    for i in range(env.observation_space.n):\n",
    "        #构建并初始化Q表\n",
    "        Q=np.zeros(env.action_space.n)\n",
    "        #计算所有动作的Q值\n",
    "        for a in range(env.action_space.n):\n",
    "            for n in env.P[i][a]:\n",
    "                trans_prob, next_state, reward_prob, _ = n\n",
    "                Q[a]=Q[a]+(trans_prob * (reward_prob + Gama * Value_Table[next_state]))\n",
    "        #通过贝尔曼最优方程得到最佳动作\n",
    "        Policy[i]=np.argmax(Q)\n",
    "    return Policy\n",
    "\n",
    "def Policy_Itreration(env,Gama):\n",
    "    Gama=1.0\n",
    "    #初始化\n",
    "    O_Policy=np.zeros(env.observation_space.n)\n",
    "    #开始迭代并检查是否已收敛\n",
    "    for i in range(Iterations):\n",
    "        #价值函数计算\n",
    "        Value_Fun=Value_Function(O_Policy,Gama)\n",
    "        #新策略\n",
    "        N_Policy=Get_Policy(Value_Fun,Gama)\n",
    "        #检查是否已收敛\n",
    "        if(np.all(O_Policy==N_Policy)):\n",
    "            print('迭代次数为%d'%(i+1))\n",
    "            break\n",
    "        O_Policy=N_Policy\n",
    "    return N_Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "得到的策略如下\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'FrozenLakeEnv' object has no attribute 'ns'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25240/3214383558.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#输出策略\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'得到的策略如下'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPolicy_Itreration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mGama\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25240/4149401609.py\u001b[0m in \u001b[0;36mPolicy_Itreration\u001b[1;34m(env, Gama)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mIterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;31m#价值函数计算\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[0mValue_Fun\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mValue_Function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mO_Policy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mGama\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[1;31m#新策略\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mN_Policy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mGet_Policy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mValue_Fun\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mGama\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25240/4149401609.py\u001b[0m in \u001b[0;36mValue_Function\u001b[1;34m(Policy, Gama)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mValue_Function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPolicy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mGama\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m#初始化价值表\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mValue_Table\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"attempted to get missing private attribute '{name}'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"attempted to get missing private attribute '{name}'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'FrozenLakeEnv' object has no attribute 'ns'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#输出策略\n",
    "print('得到的策略如下')\n",
    "print(Policy_Itreration(env,Gama))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
